---
layout: "post"
title: "on learning linear regression"
date: "2019-01-28 07:40"
---
For my second post to this blog and my second project at Metis, I learned about and implemented linear regression. The flow of the project was first we scraped data from the web, cleaned the data, performed EDA, and then modeled data using linear regression and several other predictive regression techniques.

My project was to use data and rookie drafts from 2005 to 2014 and use statistical and salary data to see if I could use performance statistics from their first year to predict their salary in their fifth year. The reason I used fifth year was because my EDA showed that their is a significant bump in NBA salaries in the fifth year of a player’s career, which I’ve read coincides with either a new contract with a higher salary or the extension of a rookie contract with a pay raise in the player’s fifth year. Of course many players weren’t playing in the NBA in their fifth year, so my final data set was significantly smaller than the size of the original draft class.

I also chose to use three different datasets for my analysis, including their total statistics across the whole season, their aggregated statistics per game, and their aggregated statistics across 36-minute periods in which they played. I also used more advanced statistics across their rookie season.

The scraping was done using BeautifulSoup, a Python module takes uses HTML pulled from the web, cleans it up, and parses through it. It allows you to look for HTML data using a variety of search parameters, including tags, ids, classes, and attributes. I mostly pulled data from a site called `basketball-reference.com`, but used web tables from ESPN’s website as well. My final tables were a table for each draft class from the first to sixtieth pick, a table for each dataset for all NBA players from the years 2005 to 2019, and a table which included all NBA salaries for those same years.

The cleaning process involved stripping out all of the null and duplicate rows in each table and removing the series of rows that restated each table column within the data to improve readability for web readers. I also reformatted numerical values to Python recognizable `ints` and `floats` so I could perform my analysis. Stripping out asterisks and various other characters from the data was also necessary. The salaries actually came in as currency-formatted strings, so converting theses to integers involved using regular expressions to parse through each salary and extract numerical digits and then joining them together to create a numerical value.

The final step in this process was to merge all of the tables together. I wanted to make a table that included all rookies who were still playing in the NBA after five years, their first-season statistics, and their fifth-season salary. This involved first merging all of the statistics tables with the rookie tables and doing an inner join to remove all non-rookie statistics. I then merged this table with the salaries table using an inner join to only get rookies who were still playing in their fifth year. This concluded my cleaning process and allowed me to move onto exploratory data analysis and modeling.

My EDA consisted of looking at correlation tables for all of the numerical statistics against salaries. Their were too many features to do any sort of per-feature plotting at this point such as a correlation heat map or a pair plot, so I decided to perform a modeling step first and then later search for any sort of multicollinearity. I also used various histograms to look for log-normal distributions and created plots to confirm my intuition that NBA salaries increased in the fifth season.

The first modeling step was to create various linear regression models to see if any of my scores for my three datasets were particularly good and look for initial correlation. I performed three linear regressions for the basic datasets, another for just the advanced statistics, and three more where I merged the basic statistics with the advanced statistics. The scoring metric I used for most of my analysis was the coefficient of determination, or r-squared. None of my regression models had particularly good r-squared values at this point. Some of them were even negative, indicating no relationship between the data and salaries.

My next step was to try adjusting the salaries to account for their logarithmic distribution and also trying this for various features that appeared to have log-normal distributions. None of these improved my r-squared scores. I also tried 2-degree polynomial regression involving squaring all of my features and then creating interaction terms using scikit-learn’s PolynomialFeatures setting. This actually made my r-squared scores worse.

It was clear from comparing my cross-validation r-squared scores with the test ones that I was overfitting the model and needed to perform some regularization to reduce this. I decided to use lasso regression to zero out some of my variables and simplify my model. I decided to do this for all three datasets and look for common features between them. After doing this, I was left with nine significant features. My r-squared scores for the lasso regressions were not particularly good, and I was beginning to think that there simply wasn’t enough data or not a strong enough relationship here to build a highly predictive model.

My next step was to run linear regression models against the three datasets using only the significant features. My r-squared scores increased slightly, and the biggest increase was for the 36-minute dataset. I decided to use this for my final model. My final step was plotting a least angle regression for this dataset to order the significance of the features.

Overall, this project taught me a significant amount about linear regression and other forms of regression prediction including lasso and ridge regression. I’m sure this will prove highly useful in the future.
